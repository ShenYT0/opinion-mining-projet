{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@zubairashfaque/sentiment-analysis-with-naive-bayes-algorithm-a31021764fb4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts: well there are thousands of international students here illegally so we gotta ramp it up\n",
      "labels: NEG\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open('../../data/clean/reddit_sentiments.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    has_header = csv.Sniffer().has_header(csvfile.read(1024))\n",
    "    csvfile.seek(0) \n",
    "    if has_header:\n",
    "        next(reader)  # just skip the header\n",
    "    for row in reader:\n",
    "        texts.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "print(\"texts:\", texts[0])\n",
    "print(\"labels:\", labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment sentiment\n",
      "0  well there are thousands of international stud...       NEG\n",
      "1  the article said dude needed a translator lol ...       NEG\n",
      "2           for those convicted of crimes thats good       POS\n",
      "3            good gotta bump up those rookie numbers       POS\n",
      "4                                               good       POS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/clean/reddit_sentiments.csv')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_sentiment(data, sentiment):\n",
    "    \"\"\"\n",
    "    Split the data DataFrame into separate lists based on sentiment.\n",
    "\n",
    "    Parameters:\n",
    "       data (DataFrame): The input DataFrame containing 'text' and 'sentiment' columns.\n",
    "       sentiment (str): The sentiment label to filter the data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text corresponding to the specified sentiment.\n",
    "    \"\"\"\n",
    "    return data[data['sentiment'] == sentiment]['comment'].tolist()\n",
    "\n",
    "# Assuming df is your DataFrame containing 'text' and 'sentiment' columns\n",
    "positive_data = split_data_by_sentiment(df, 'POS')\n",
    "negative_data = split_data_by_sentiment(df, 'NEG')\n",
    "neutral_data = split_data_by_sentiment(df, 'NEU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation from the text using translation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenize the text into individual words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Initialize a Porter stemmer for word stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Get a set of English stopwords from NLTK\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Apply stemming to each token and filter out stopwords\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stopwords_set]\n",
    "    \n",
    "    # Return the preprocessed tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def calculate_word_counts(textes):\n",
    "    # Initialize a defaultdict to store word counts, defaulting to 0 for unseen words\n",
    "    word_count = defaultdict(int)\n",
    "    \n",
    "    # Iterate through each text in the given list of textes\n",
    "    for text in textes:\n",
    "        # Tokenize and preprocess the text using the preprocess_text function\n",
    "        tokens = preprocess_text(text)\n",
    "        \n",
    "        # Iterate through each token in the preprocessed tokens\n",
    "        for token in tokens:\n",
    "            # Increment the count for the current token in the word_count dictionary\n",
    "            word_count[token] += 1\n",
    "    \n",
    "    # Return the word_count dictionary containing word frequencies\n",
    "    return word_count\n",
    "\n",
    "# Calculate word counts for textes with positive sentiment\n",
    "word_count_positive = calculate_word_counts(train_df[train_df['sentiment'] == 'POS']['comment'])\n",
    "\n",
    "# Calculate word counts for textes with negative sentiment\n",
    "word_count_negative = calculate_word_counts(train_df[train_df['sentiment'] == 'NEG']['comment'])\n",
    "\n",
    "# Calculate word counts for textes with neutral sentiment\n",
    "word_count_neutral = calculate_word_counts(train_df[train_df['sentiment'] == 'NEU']['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood(word_count, total_words, laplacian_smoothing=1):\n",
    "    # Create an empty dictionary to store the likelihood values\n",
    "    likelihood = {}\n",
    "    \n",
    "    # Get the number of unique words in the vocabulary\n",
    "    vocabulary_size = len(word_count)\n",
    "\n",
    "    # Iterate through each word and its corresponding count in the word_count dictionary\n",
    "    for word, count in word_count.items():\n",
    "        # Calculate the likelihood using Laplacian smoothing formula\n",
    "        # Laplacian smoothing is used to handle unseen words in training data\n",
    "        # The formula is (count + smoothing) / (total_words + smoothing * vocabulary_size)\n",
    "        likelihood[word] = (count + laplacian_smoothing) / (total_words + laplacian_smoothing * vocabulary_size)\n",
    "\n",
    "    # Return the calculated likelihood dictionary\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_positive = calculate_likelihood(word_count_positive, len(train_df[train_df['sentiment'] == 'POS']), 1)\n",
    "likelihood_negative = calculate_likelihood(word_count_negative, len(train_df[train_df['sentiment'] == 'NEG']), 1)\n",
    "likelihood_neutral = calculate_likelihood(word_count_neutral, len(train_df[train_df['sentiment'] == 'NEU']), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_log_prior(sentiment, data):\n",
    "    # Calculate the natural logarithm of the ratio of textes with the specified sentiment to the total number of textes\n",
    "    log_prior = math.log(len(data[data['sentiment'] == sentiment]) / len(data))\n",
    "    \n",
    "    # Return the calculated log prior\n",
    "    return log_prior\n",
    "\n",
    "# Calculate the log prior for textes with positive sentiment\n",
    "log_prior_positive = calculate_log_prior('POS', df)\n",
    "\n",
    "# Calculate the log prior for textes with negative sentiment\n",
    "log_prior_negative = calculate_log_prior('NEG', df)\n",
    "\n",
    "# Calculate the log prior for textes with neutral sentiment\n",
    "log_prior_neutral = calculate_log_prior('NEU', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of log-likelihood values for positive sentiment\n",
    "log_likelihood_positive = {word: math.log(prob) for word, prob in likelihood_positive.items()}\n",
    "\n",
    "# Create a dictionary of log-likelihood values for negative sentiment\n",
    "log_likelihood_negative = {word: math.log(prob) for word, prob in likelihood_negative.items()}\n",
    "\n",
    "# Create a dictionary of log-likelihood values for neutral sentiment\n",
    "log_likelihood_neutral = {word: math.log(prob) for word, prob in likelihood_neutral.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_with_scores(text, log_likelihood_positive, log_likelihood_negative, log_likelihood_neutral,\n",
    "                               log_prior_positive, log_prior_negative, log_prior_neutral):\n",
    "    # Tokenize and preprocess the input text\n",
    "    tokens = preprocess_text(text)\n",
    "\n",
    "    # Calculate the log scores for each sentiment category\n",
    "    log_score_positive = log_prior_positive + sum([log_likelihood_positive.get(token, 0) for token in tokens])\n",
    "    log_score_negative = log_prior_negative + sum([log_likelihood_negative.get(token, 0) for token in tokens])\n",
    "    log_score_neutral = log_prior_neutral + sum([log_likelihood_neutral.get(token, 0) for token in tokens])\n",
    "\n",
    "    # Store the sentiment scores in a dictionary\n",
    "    sentiment_scores = {\n",
    "        'positive': log_score_positive,\n",
    "        'negative': log_score_negative,\n",
    "        'neutral': log_score_neutral\n",
    "    }\n",
    "\n",
    "    # Determine the predicted sentiment based on the highest sentiment score\n",
    "    predicted_sentiment = max(sentiment_scores, key=sentiment_scores.get)\n",
    "    \n",
    "    # Return the predicted sentiment and the sentiment scores\n",
    "    return predicted_sentiment, sentiment_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: I love you so much!\n",
      "Predicted Sentiment: positive\n",
      "Sentiment Scores: {'positive': -11.383843163704702, 'negative': -11.663342264697556, 'neutral': -13.402463765735227}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Classify a sample text using the trained model\n",
    "text = \"I love you so much!\"\n",
    "predicted_sentiment, sentiment_scores = classify_text_with_scores(text, log_likelihood_positive, log_likelihood_negative, log_likelihood_neutral,\n",
    "                                                                    log_prior_positive, log_prior_negative, log_prior_neutral)\n",
    "\n",
    "print(\"Sample sentence:\", text)\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment)\n",
    "print(\"Sentiment Scores:\", sentiment_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
